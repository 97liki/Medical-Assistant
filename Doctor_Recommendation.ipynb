{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiykYEJgSh9d",
        "outputId": "eb24af1f-3448-4a1d-c2c5-ee2696306990"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from googlesearch import search\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Take input a disease and return the content of wikipedia's infobox for that specific disease\n",
        "\n",
        "def diseaseDetail(term):\n",
        "    diseases=[term]\n",
        "    ret=term+\"\\n\"\n",
        "    for dis in diseases:\n",
        "        # search \"disease wilipedia\" on google\n",
        "        query = dis+' wikipedia'\n",
        "        for sr in search(query,tld=\"co.in\",stop=10,pause=0.5):\n",
        "            # open wikipedia link\n",
        "            match=re.search(r'wikipedia',sr)\n",
        "            filled = 0\n",
        "            if match:\n",
        "                wiki = requests.get(sr,verify=False)\n",
        "                soup = BeautifulSoup(wiki.content, 'html5lib')\n",
        "                # Fetch HTML code for 'infobox'\n",
        "                info_table = soup.find(\"table\", {\"class\":\"infobox\"})\n",
        "                if info_table is not None:\n",
        "                    # Preprocess contents of infobox\n",
        "                    for row in info_table.find_all(\"tr\"):\n",
        "                        data=row.find(\"th\",{\"scope\":\"row\"})\n",
        "                        if data is not None:\n",
        "                            symptom=str(row.find(\"td\"))\n",
        "                            symptom = symptom.replace('.','')\n",
        "                            symptom = symptom.replace(';',',')\n",
        "                            symptom = symptom.replace('<b>','<b> \\n')\n",
        "                            symptom=re.sub(r'<a.*?>','',symptom) # Remove hyperlink\n",
        "                            symptom=re.sub(r'</a>','',symptom) # Remove hyperlink\n",
        "                            symptom=re.sub(r'<[^<]+?>',' ',symptom) # All the tags\n",
        "                            symptom=re.sub(r'\\[.*\\]','',symptom) # Remove citation text\n",
        "                            symptom=symptom.replace(\"&gt\",\">\")\n",
        "                            ret+=data.get_text()+\" - \"+symptom+\"\\n\"\n",
        "#                            print(data.get_text(),\"-\",symptom)\n",
        "                            filled = 1\n",
        "                if filled:\n",
        "                    break\n",
        "    return ret"
      ],
      "metadata": {
        "id": "htsFCAHeZ6uW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9yaeP-7Sc_H"
      },
      "source": [
        "***Run all the cells***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpY-ArzB1rH-"
      },
      "source": [
        "# **Disease Detection using Symptoms and Doctor recommendation**\n",
        "\n",
        "This notebook contains code to detect disease using the symptoms entered and selected by the user and recommends the appropriate doctors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jcDTmIyqctTq"
      },
      "outputs": [],
      "source": [
        "# Predicts diseases based on the symptoms entered and selected by the user.\n",
        "# importing all necessary libraries\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from statistics import mean\n",
        "from nltk.corpus import wordnet\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from itertools import combinations\n",
        "from time import time\n",
        "from collections import Counter\n",
        "import operator\n",
        "from xgboost import XGBClassifier\n",
        "import math\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "warnings.simplefilter(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oab3lA7fj8GZ"
      },
      "source": [
        "Download resources required for NLTK pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUcEIGoij4o8",
        "outputId": "e9c1fb78-d5c1-48eb-972c-0db46e6807a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsuWDbumeSco"
      },
      "source": [
        "**synonyms function** finds the synonymous terms of a symptom entered by the user.\n",
        "\n",
        "This is necessary as the user may use a term for a symptom which may be different from the one present in dataset.\n",
        "This improves the accuracy by reducing the wrong predictions even when symptoms for a disease are entered slightly different than the ones on which model is trained.\n",
        "\n",
        "*Synonyms are searched on Thesaurus.com and NLTK Wordnet*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "DhrzSJPadBwH"
      },
      "outputs": [],
      "source": [
        "# returns the list of synonyms of the input word from thesaurus.com (https://www.thesaurus.com/) and wordnet (https://www.nltk.org/howto/wordnet.html)\n",
        "def synonyms(term):\n",
        "    synonyms = []\n",
        "    response = requests.get('https://www.thesaurus.com/browse/{}'.format(term))\n",
        "    soup = BeautifulSoup(response.content,  \"html.parser\")\n",
        "    try:\n",
        "        container=soup.find('section', {'class': 'MainContentContainer'})\n",
        "        row=container.find('div',{'class':'css-191l5o0-ClassicContentCard'})\n",
        "        row = row.find_all('li')\n",
        "        for x in row:\n",
        "            synonyms.append(x.get_text())\n",
        "    except:\n",
        "        None\n",
        "    for syn in wordnet.synsets(term):\n",
        "        synonyms+=syn.lemma_names()\n",
        "    return set(synonyms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JwigX34ldGPl"
      },
      "outputs": [],
      "source": [
        "# utlities for pre-processing\n",
        "stop_words = stopwords.words('english')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "splitter = RegexpTokenizer(r'\\w+')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZTXyRhNgN_O"
      },
      "source": [
        "Disease Combination dataset contains the combinations for each of the disease present in dataset as practically it is often observed that it is not necessary for a person to have a disease when all the symptoms are faced by the patient or the user.\n",
        "\n",
        "*To tackle this problem, combinations are made with the symptoms for each disease.*\n",
        "\n",
        " **This increases the size of the data exponentially and helps the model to predict the disease with much better accuracy.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1LSI08aiDTn"
      },
      "source": [
        "*df_comb -> Dataframe consisting of dataset generated by combining symptoms for each disease.*\n",
        "\n",
        "*df_norm -> Dataframe consisting of dataset which contains a single row for each diseases with all the symptoms for that corresponding disease.*\n",
        "\n",
        "**Dataset contains 261 diseases and their symptoms**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5fzGWm5NdIkN"
      },
      "outputs": [],
      "source": [
        "df_comb = pd.read_csv(\"/content/drive/MyDrive/Medical-Assistant-main/Dataset/dis_sym_dataset_comb.csv\") # Disease combination\n",
        "df_norm = pd.read_csv(\"/content/drive/MyDrive/Medical-Assistant-main/Dataset/dis_sym_dataset_norm.csv\") # Individual Disease\n",
        "\n",
        "X = df_comb.iloc[:, 1:]\n",
        "Y = df_comb.iloc[:, 0:1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4fuvOxOhR6v"
      },
      "source": [
        "Using **Logistic Regression (LR) Classifier** as it gives better accuracy compared to other classification models as observed in the comparison of model accuracies in Model_latest.py\n",
        "\n",
        "Cross validation is done on dataset with cv = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Njmbkf6IdKwt"
      },
      "outputs": [],
      "source": [
        "lr = LogisticRegression()\n",
        "lr = lr.fit(X, Y)\n",
        "scores = cross_val_score(lr, X, Y, cv=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Gvck32ifdVZV"
      },
      "outputs": [],
      "source": [
        "X = df_norm.iloc[:, 1:]\n",
        "Y = df_norm.iloc[:, 0:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0ppiBHtudX1O"
      },
      "outputs": [],
      "source": [
        "# List of symptoms\n",
        "dataset_symptoms = list(X.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNytuZen1Ij3"
      },
      "source": [
        "# Symptoms initially taken from user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eRjwZhQdbxN",
        "outputId": "b1850149-b9ce-451f-a9e9-3fce6f65b5a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter symptoms separated by comma(,):\n",
            "cough, cold\n"
          ]
        }
      ],
      "source": [
        "# Taking symptoms from user as input\n",
        "user_symptoms = str(input(\"Please enter symptoms separated by comma(,):\\n\")).lower().split(',')\n",
        "# Preprocessing the input symptoms\n",
        "processed_user_symptoms=[]\n",
        "for sym in user_symptoms:\n",
        "    sym=sym.strip()\n",
        "    sym=sym.replace('-',' ')\n",
        "    sym=sym.replace(\"'\",'')\n",
        "    sym = ' '.join([lemmatizer.lemmatize(word) for word in splitter.tokenize(sym)])\n",
        "    processed_user_symptoms.append(sym)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuN02omLSc_Q",
        "outputId": "0ef94798-99df-4fb0-ad97-96f9558b0259"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cough', ' cold']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "user_symptoms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCAJTUngi_V_"
      },
      "source": [
        "Pre-processing on symptoms entered by user is done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pewx4v_jdcbV",
        "outputId": "25a1a0c4-2811-441d-af0c-e9fa99930588"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After query expansion done by using the symptoms entered\n",
            "['cough coughing', 'stale cold-blooded frigidity common cold coldness cold dusty low temperature moth-eaten frigid frigidness inhuman insensate']\n"
          ]
        }
      ],
      "source": [
        "# Taking each user symptom and finding all its synonyms and appending it to the pre-processed symptom string\n",
        "user_symptoms = []\n",
        "for user_sym in processed_user_symptoms:\n",
        "    user_sym = user_sym.split()\n",
        "    str_sym = set()\n",
        "    for comb in range(1, len(user_sym)+1):\n",
        "        for subset in combinations(user_sym, comb):\n",
        "            subset=' '.join(subset)\n",
        "            subset = synonyms(subset)\n",
        "            str_sym.update(subset)\n",
        "    str_sym.add(' '.join(user_sym))\n",
        "    user_symptoms.append(' '.join(str_sym).replace('_',' '))\n",
        "# query expansion performed by joining synonyms found for each symptoms initially entered\n",
        "print(\"After query expansion done by using the symptoms entered\")\n",
        "print(user_symptoms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sPyVlJIjdv2"
      },
      "source": [
        "The below procedure is performed in order to show the symptom synonmys found for the symptoms entered by the user.\n",
        "\n",
        "The symptom synonyms and user symptoms are matched with the symptoms present in dataset. Only the symptoms which matches the symptoms present in dataset are shown back to the user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "qVnrRYXpdelN"
      },
      "outputs": [],
      "source": [
        "# Loop over all the symptoms in dataset and check its similarity score to the synonym string of the user-input\n",
        "# symptoms. If similarity>0.5, add the symptom to the final list\n",
        "found_symptoms = set()\n",
        "for idx, data_sym in enumerate(dataset_symptoms):\n",
        "    data_sym_split=data_sym.split()\n",
        "    for user_sym in user_symptoms:\n",
        "        count=0\n",
        "        for symp in data_sym_split:\n",
        "            if symp in user_sym.split():\n",
        "                count+=1\n",
        "        if count/len(data_sym_split)>0.5:\n",
        "            found_symptoms.add(data_sym)\n",
        "found_symptoms = list(found_symptoms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5g0jKPRfj-dP"
      },
      "source": [
        "## **Prompt the user to select the relevant symptoms by entering the corresponding indices.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piRNP4WSdiCG",
        "outputId": "a4c9991b-bba5-498f-f6b1-1a1dff092e16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top matching symptoms from your search!\n",
            "0 : coughing\n"
          ]
        }
      ],
      "source": [
        "# Print all found symptoms\n",
        "print(\"Top matching symptoms from your search!\")\n",
        "for idx, symp in enumerate(found_symptoms):\n",
        "    print(idx,\":\",symp)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zOeGDQVSc_R",
        "outputId": "5ed908ac-ee1c-471b-e1b3-8ba019f6aa2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Please select the relevant symptoms. Enter indices (separated-space):\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "# Show the related symptoms found in the dataset and ask user to select among them\n",
        "select_list = input(\"\\nPlease select the relevant symptoms. Enter indices (separated-space):\\n\").split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ra8lGaAgSc_R"
      },
      "outputs": [],
      "source": [
        "# Find other relevant symptoms from the dataset based on user symptoms based on the highest co-occurance with the\n",
        "# ones that is input by the user\n",
        "dis_list = set()\n",
        "final_symp = []\n",
        "counter_list = []\n",
        "for idx in select_list:\n",
        "    symp=found_symptoms[int(idx)]\n",
        "    final_symp.append(symp)\n",
        "    dis_list.update(set(df_norm[df_norm[symp]==1]['label_dis']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "U_5Zxny3Sc_R"
      },
      "outputs": [],
      "source": [
        "for dis in dis_list:\n",
        "    row = df_norm.loc[df_norm['label_dis'] == dis].values.tolist()\n",
        "    row[0].pop(0)\n",
        "    for idx,val in enumerate(row[0]):\n",
        "        if val!=0 and dataset_symptoms[idx] not in final_symp:\n",
        "            counter_list.append(dataset_symptoms[idx])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO2mW3K4oIz9"
      },
      "source": [
        "## To find symptoms which generally co-occur, for example with symptoms like cough, headache generally happens hence they co-occur."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "pjq2q16OdlHe"
      },
      "outputs": [],
      "source": [
        "# Symptoms that co-occur with the ones selected by user\n",
        "dict_symp = dict(Counter(counter_list))\n",
        "dict_symp_tup = sorted(dict_symp.items(), key=operator.itemgetter(1),reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_irt5GF2Sc_R",
        "outputId": "d530d62f-2864-4034-a665-f08feb70f747"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) feeling tired\n",
            "2) fever\n",
            "3) headache\n",
            "4) muscle joint pain\n",
            "5) runny nose\n"
          ]
        }
      ],
      "source": [
        "dict_symp = dict(Counter(counter_list))\n",
        "# Sorting the list of tuples based on the count in descending order\n",
        "dict_symp_tup = sorted(dict_symp.items(), key=operator.itemgetter(1), reverse=True)\n",
        "\n",
        "# Printing only the top 5 symptoms with their index numbers\n",
        "for index, (symptom, count) in enumerate(dict_symp_tup[:5]):\n",
        "    print(f\"{index + 1}) {symptom}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjmBiLuGpEfH"
      },
      "source": [
        "## User is presented with a list of co-occuring symptoms to select from and is performed iteratively to recommend more possible symptoms based on the similarity to the previously entered symptoms.\n",
        "\n",
        "As the co-occuring symptoms can be in overwhelming numbers, only the top 5 are recommended to the user from which user can select the symptoms.\n",
        "\n",
        "If user does not have any of those 5 symptoms and wants to see the next 5, he can do so by giving input as -1.\n",
        "\n",
        "To stop the recommendation, user needs to give input as \"No\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neTz-oNVdn0N",
        "outputId": "1968f79f-b7cd-4eba-802b-9948469814c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Common co-occuring symptoms:\n",
            "0 : feeling tired\n",
            "1 : fever\n",
            "2 : headache\n",
            "3 : muscle joint pain\n",
            "4 : runny nose\n",
            "Do you have have of the symptoms from the above? If Yes, enter the indices (space-separated), 'no' to stop, '-1' to skip:\n",
            "0\n",
            "\n",
            "Common co-occuring symptoms:\n",
            "0 : sore throat\n",
            "1 : chest tightness\n",
            "2 : recurring episode wheezing\n",
            "3 : shortness breath\n",
            "Do you have have of the symptoms from the above? If Yes, enter the indices (space-separated), 'no' to stop, '-1' to skip:\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "# Iteratively, suggest top co-occuring symptoms to the user and ask to select the ones applicable\n",
        "found_symptoms=[]\n",
        "count=0\n",
        "\n",
        "for tup in dict_symp_tup:\n",
        "    count+=1\n",
        "    found_symptoms.append(tup[0])\n",
        "    if count%5==0 or count==len(dict_symp_tup):\n",
        "        print(\"\\nCommon co-occuring symptoms:\")\n",
        "        for idx,ele in enumerate(found_symptoms):\n",
        "            print(idx,\":\",ele)\n",
        "        select_list = input(\"Do you have have of the symptoms from the above? If Yes, enter the indices (space-separated), 'no' to stop, '-1' to skip:\\n\").lower().split();\n",
        "        if select_list[0]=='no':\n",
        "            break\n",
        "        if select_list[0]=='-1':\n",
        "            found_symptoms = []\n",
        "            continue\n",
        "        for idx in select_list:\n",
        "            final_symp.append(found_symptoms[int(idx)])\n",
        "        found_symptoms = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nI5taHc8pfY3"
      },
      "source": [
        "Final Symptom list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jIiVsbBdpg-",
        "outputId": "93a5cba8-a868-4e4c-88d2-c9ed92842c7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final list of Symptoms that will be used for prediction:\n",
            "coughing\n",
            "feeling tired\n",
            "sore throat\n"
          ]
        }
      ],
      "source": [
        "# Create query vector based on symptoms selected by the user\n",
        "print(\"\\nFinal list of Symptoms that will be used for prediction:\")\n",
        "sample_x = [0 for x in range(0,len(dataset_symptoms))]\n",
        "for val in final_symp:\n",
        "    print(val)\n",
        "    sample_x[dataset_symptoms.index(val)]=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9MbvRV_phpv"
      },
      "source": [
        "Prediction of disease is done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "8yWRutt2drbt"
      },
      "outputs": [],
      "source": [
        "# Predict disease\n",
        "lr = LogisticRegression()\n",
        "lr = lr.fit(X, Y)\n",
        "prediction = lr.predict_proba([sample_x])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpwualbypkNl"
      },
      "source": [
        "Show top k diseases and their probabilities to the user.\n",
        "\n",
        "K in this case is 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "bVcksjqSdt61"
      },
      "outputs": [],
      "source": [
        "k = 10\n",
        "diseases = list(set(Y['label_dis']))\n",
        "diseases.sort()\n",
        "topk = prediction[0].argsort()[-k:][::-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wH7-UgGtSc_g",
        "outputId": "a607c9a8-4476-4d5d-d59e-821e3aa97894"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Diseases Predicted:\n",
            "Influenza\n",
            "Rubella\n",
            "Brucellosis\n",
            "Hepatitis D\n",
            "Fibromyalgia\n",
            "Asthma\n",
            "Congestive heart disease\n",
            "Botulism\n",
            "Strep throat\n",
            "Anaemia\n"
          ]
        }
      ],
      "source": [
        "# Assuming 'prediction' is the output of your model, giving a probability for each disease\n",
        "topk = prediction[0].argsort()[-k:][::-1]\n",
        "\n",
        "most_probable_diseases=[]\n",
        "\n",
        "# Printing diseases corresponding to the indices in topk\n",
        "print(\"Top Diseases Predicted:\")\n",
        "for idx in topk:\n",
        "    most_probable_diseases.append(diseases[idx])\n",
        "    print(diseases[idx])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrJZJIbESc_h"
      },
      "source": [
        "***Suggesting doctors based on most probable diseases***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ngKiONbDSc_h"
      },
      "outputs": [],
      "source": [
        "doctors=pd.read_csv('/content/drive/MyDrive/Medical-Assistant-main/doctors.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "60RkWSGRSc_h"
      },
      "outputs": [],
      "source": [
        "# Split the 'Specialization' entries and expand them into a list\n",
        "specializations = doctors['Specialization'].str.split(',').explode()\n",
        "\n",
        "# Split the 'Specialization' entries and expand them into a list\n",
        "specializations = doctors['Specialization'].str.split(',').explode()\n",
        "\n",
        "# Count occurrences of each specialization\n",
        "specialization_counts = specializations.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEm19hBTSc_i"
      },
      "source": [
        "# Disease to Sepecialiation mapping using GPT3.5 Turbo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "KtvjNkU6Sc_i"
      },
      "outputs": [],
      "source": [
        "# Mapping diseases to medical specializations\n",
        "disease_to_specialization = {\n",
        "    'Abscess': 'General Surgeon',\n",
        "    'Acquired Capillary Haemangioma of Eyelid': 'Dermatologist',\n",
        "    'Acquired Immuno Deficiency Syndrome': 'Infectious Diseases',\n",
        "    'Acute encephalitis syndrome': 'Neurologist',\n",
        "    'Adult Inclusion Conjunctivitis': 'Eye Specialist',\n",
        "    'Alcohol Abuse and Alcoholism': 'Psychiatrist',\n",
        "    'Alopecia (hair loss)': 'Dermatologist',\n",
        "    'Alzheimer': 'Neurologist',\n",
        "    'Amaurosis Fugax': 'Ophthalmologist',\n",
        "    'Amblyopia': 'Ophthalmologist',\n",
        "    'Amoebiasis': 'Gastroenterologist',\n",
        "    'Anaemia': 'Hematologist',\n",
        "    'Aniseikonia': 'Ophthalmologist',\n",
        "    'Anisometropia': 'Ophthalmologist',\n",
        "    'Antepartum hemorrhage (Bleeding in late pregnancy)': 'Gynecologist',\n",
        "    'Anthrax': 'Infectious Diseases',\n",
        "    'Anxiety': 'Psychiatrist',\n",
        "    'Appendicitis': 'General Surgeon',\n",
        "    'Arthritis': 'Rheumatologist',\n",
        "    'Asbestos-related diseases': 'Pulmonologist / Lung Specialist',\n",
        "    'Aseptic meningitis': 'Neurologist',\n",
        "    'Asthma': 'Pulmonologist / Lung Specialist',\n",
        "    'Astigmatism': 'Ophthalmologist',\n",
        "    'Atrophy': 'Neurologist',\n",
        "    'Autism': 'Pediatrician',\n",
        "    'Bad Breath (Halitosis)': 'General Physician',\n",
        "    \"Bell's Palsy\": 'Neurologist',\n",
        "    'Beriberi': 'Nutritionist',\n",
        "    'Black Death': 'Infectious Diseases',\n",
        "    'Bleeding Gums': 'Dentist',\n",
        "    'Blindness': 'Ophthalmologist',\n",
        "    'Botulism': 'Infectious Diseases',\n",
        "    'Brain Tumour': 'Neuro Surgeon',\n",
        "    'Breast Cancer / Carcinoma': 'Oncologist',\n",
        "    'Bronchitis': 'Pulmonologist / Lung Specialist',\n",
        "    'Brucellosis': 'Infectious Diseases',\n",
        "    'Bubonic plague': 'Infectious Diseases',\n",
        "    'Bunion': 'Orthopedic Surgeon',\n",
        "    'Burns': 'General Surgeon',\n",
        "    'Calculi': 'Urologist',\n",
        "    'Campylobacter infection': 'Gastroenterologist',\n",
        "    'Cancer': 'Oncologist',\n",
        "    'Candidiasis': 'Infectious Diseases',\n",
        "    'Carbon monoxide poisoning': 'Emergency Medicine',\n",
        "    'Carpal Tunnel Syndrome': 'Orthopedic Surgeon',\n",
        "    'Cavities': 'Dentist',\n",
        "    'Celiacs disease': 'Gastroenterologist',\n",
        "    'Cerebral palsy': 'Pediatric Neurologist',\n",
        "    'Chagas disease': 'Infectious Diseases',\n",
        "    'Chalazion': 'Ophthalmologist',\n",
        "    'Chickenpox': 'Pediatrician',\n",
        "    'Chikungunya Fever': 'Infectious Diseases',\n",
        "    'Childhood Exotropia': 'Ophthalmologist',\n",
        "    'Chlamydia': 'Sexologist',\n",
        "    'Cholera': 'Infectious Diseases',\n",
        "    'Chorea': 'Neurologist',\n",
        "    'Chronic fatigue syndrome': 'Internal Medicine Specialist',\n",
        "    'Chronic obstructive pulmonary disease (COPD)': 'Pulmonologist / Lung Specialist',\n",
        "    'Cleft Lip and Cleft Palate': 'Plastic Surgeon',\n",
        "    'Colitis': 'Gastroenterologist',\n",
        "    'Colorectal Cancer': 'Oncologist',\n",
        "    'Common cold': 'General Physician',\n",
        "    'Condyloma': 'Dermatologist',\n",
        "    'Congenital anomalies (birth defects)': 'Geneticist',\n",
        "    'Congestive heart disease': 'Cardiologist',\n",
        "    'Corneal Abrasion': 'Ophthalmologist',\n",
        "    'Coronary Heart Disease': 'Cardiologist',\n",
        "    'Coronavirus disease 2019 (COVID-19)': 'Infectious Diseases',\n",
        "    'Cough': 'General Physician',\n",
        "    'Crimean Congo haemorrhagic fever (CCHF)': 'Infectious Diseases',\n",
        "    'Dehydration': 'General Physician',\n",
        "    'Dementia': 'Neurologist',\n",
        "    'Dengue': 'Infectious Diseases',\n",
        "    'Diabetes Mellitus': 'Endocrinologist',\n",
        "    'Diabetic Retinopathy': 'Ophthalmologist',\n",
        "    'Diarrhea': 'Gastroenterologist',\n",
        "    'Diphtheria': 'Infectious Diseases',\n",
        "    \"Down's Syndrome\": 'Geneticist',\n",
        "    'Dracunculiasis (guinea-worm disease)': 'Infectious Diseases',\n",
        "    'Dysentery': 'Gastroenterologist',\n",
        "    'Ear infection': 'Ent Specialist',\n",
        "    'Early pregnancy loss': 'Gynecologist',\n",
        "    'Ebola': 'Infectious Diseases',\n",
        "    'Eclampsia': 'Gynecologist',\n",
        "    'Ectopic pregnancy': 'Gynecologist',\n",
        "    'Eczema': 'Dermatologist',\n",
        "    'Endometriosis': 'Gynecologist',\n",
        "    'Epilepsy': 'Neurologist',\n",
        "    'Fibroids': 'Gynecologist',\n",
        "    'Fibromyalgia': 'Rheumatologist',\n",
        "    'Food Poisoning': 'Gastroenterologist',\n",
        "    'Frost Bite': 'General Surgeon',\n",
        "    'GERD': 'Gastroenterologist',\n",
        "    'Gaming disorder': 'Psychiatrist',\n",
        "    'Gangrene': 'General Surgeon',\n",
        "    'Gastroenteritis': 'Gastroenterologist',\n",
        "    'Genital herpes': 'Dermatologist',\n",
        "    'Glaucoma': 'Ophthalmologist',\n",
        "    'Goitre': 'Endocrinologist',\n",
        "    'Gonorrhea': 'Sexologist',\n",
        "    'Guillain-Barré syndrome': 'Neurologist',\n",
        "    'Haemophilia': 'Hematologist',\n",
        "    'Hand, Foot and Mouth Disease': 'Pediatrician',\n",
        "    'Heat-Related Illnesses and Heat waves': 'Emergency Medicine',\n",
        "    'Hepatitis': 'Hepatologist',\n",
        "    'Hepatitis A': 'Hepatologist',\n",
        "    'Hepatitis B': 'Hepatologist',\n",
        "    'Hepatitis C': 'Hepatologist',\n",
        "    'Hepatitis D': 'Hepatologist',\n",
        "    'Hepatitis E': 'Hepatologist',\n",
        "    'Herpes Simplex': 'Dermatologist',\n",
        "    'High risk pregnancy': 'Gynecologist',\n",
        "    'Human papillomavirus': 'Dermatologist',\n",
        "    'Hypermetropia': 'Ophthalmologist',\n",
        "    'Hyperthyroidism': 'Endocrinologist',\n",
        "    'Hypothyroid': 'Endocrinologist',\n",
        "    'Hypotonia': 'Pediatrician',\n",
        "    'Impetigo': 'Dermatologist',\n",
        "    'Inflammatory Bowel Disease': 'Gastroenterologist',\n",
        "    'Influenza': 'General Physician',\n",
        "    'Insomnia': 'Psychiatrist',\n",
        "    'Interstitial cystitis': 'Urologist',\n",
        "    'Iritis': 'Ophthalmologist',\n",
        "    'Iron Deficiency Anemia': 'Hematologist',\n",
        "    'Irritable bowel syndrome': 'Gastroenterologist',\n",
        "    'Japanese Encephalitis': 'Infectious Diseases',\n",
        "    'Jaundice': 'Hepatologist',\n",
        "    'Kala-azar/ Leishmaniasis': 'Infectious Diseases',\n",
        "    'Kaposi’s Sarcoma': 'Oncologist',\n",
        "    'Keratoconjunctivitis Sicca (Dry eye syndrome)': 'Ophthalmologist',\n",
        "    'Keratoconus': 'Ophthalmologist',\n",
        "    'Kuru': 'Neurologist',\n",
        "    'Laryngitis': 'Ent Specialist',\n",
        "    'Lead poisoning': 'Toxicologist',\n",
        "    'Legionellosis': 'Infectious Diseases',\n",
        "    'Leprosy': 'Dermatologist',\n",
        "    'Leptospirosis': 'Infectious Diseases',\n",
        "    'Leukemia': 'Hematologist',\n",
        "    'Lice': 'Dermatologist',\n",
        "    'Lung cancer': 'Oncologist',\n",
        "    'Lupus erythematosus': 'Rheumatologist',\n",
        "    'Lyme disease': 'Infectious Diseases',\n",
        "    'Lymphoma': 'Oncologist',\n",
        "    'Mad cow disease': 'Neurologist',\n",
        "    'Malaria': 'Infectious Diseases',\n",
        "    'Marburg fever': 'Infectious Diseases',\n",
        "    'Mastitis': 'Gynecologist',\n",
        "    'Measles': 'Pediatrician',\n",
        "    'Melanoma': 'Oncologist',\n",
        "    'Middle East respiratory syndrome coronavirus (MERS‐CoV)': 'Infectious Diseases',\n",
        "    'Migraine': 'Neurologist',\n",
        "    'Mononucleosis': 'Infectious Diseases',\n",
        "    'Mouth Breathing': 'Ent Specialist',\n",
        "    'Multiple myeloma': 'Oncologist',\n",
        "    'Multiple sclerosis': 'Neurologist',\n",
        "    'Mumps': 'Pediatrician',\n",
        "    'Muscular dystrophy': 'Neurologist',\n",
        "    'Myasthenia gravis': 'Neurologist',\n",
        "    'Myelitis': 'Neurologist',\n",
        "    'Myocardial Infarction (Heart Attack)': 'Cardiologist',\n",
        "    'Myopia': 'Ophthalmologist',\n",
        "    'Narcolepsy': 'Neurologist',\n",
        "    'Nasal Polyps': 'Ent Specialist',\n",
        "    'Nausea and Vomiting of Pregnancy and  Hyperemesis gravidarum': 'Gynecologist',\n",
        "    'Necrotizing Fasciitis': 'General Surgeon',\n",
        "    'Neonatal Respiratory Disease Syndrome(NRDS)': 'Pediatrician',\n",
        "    'Neoplasm': 'Oncologist',\n",
        "    'Neuralgia': 'Neurologist',\n",
        "    'Nipah virus infection': 'Infectious Diseases',\n",
        "    'Obesity': 'Nutritionist',\n",
        "    'Obsessive Compulsive Disorder': 'Psychiatrist',\n",
        "    'Oral Cancer': 'Oncologist',\n",
        "    'Orbital Dermoid': 'Ophthalmologist',\n",
        "    'Osteoarthritis': 'Orthopedic Surgeon',\n",
        "    'Osteomyelitis': 'Orthopedic Surgeon',\n",
        "    'Osteoporosis': 'Orthopedic Surgeon',\n",
        "    'Paratyphoid fever': 'Infectious Diseases',\n",
        "    \"Parkinson's Disease\": 'Neurologist',\n",
        "    'Pelvic inflammatory disease': 'Gynecologist',\n",
        "    'Perennial Allergic Conjunctivitis': 'Allergy Specialist',\n",
        "    'Pericarditis': 'Cardiologist',\n",
        "    'Peritonitis': 'General Surgeon',\n",
        "    'Pinguecula': 'Ophthalmologist',\n",
        "    'Pneumonia': 'Pulmonologist / Lung Specialist',\n",
        "    'Poliomyelitis': 'Pediatrician',\n",
        "    'Polycystic ovary syndrome (PCOS)': 'Endocrinologist',\n",
        "    'Porphyria': 'Dermatologist',\n",
        "    'Post Menopausal Bleeding': 'Gynecologist',\n",
        "    'Post-herpetic neuralgia': 'Pain Specialist',\n",
        "    'Postpartum depression/ Perinatal depression': 'Psychiatrist',\n",
        "    'Preeclampsia': 'Gynecologist',\n",
        "    'Premenstrual syndrome': 'Gynecologist',\n",
        "    'Presbyopia': 'Ophthalmologist',\n",
        "    'Preterm birth': 'Gynecologist',\n",
        "    'Progeria': 'Geneticist',\n",
        "    'Psoriasis': 'Dermatologist',\n",
        "    'Puerperal sepsis': 'Gynecologist',\n",
        "    'Pulmonary embolism': 'Pulmonologist / Lung Specialist',\n",
        "    'Ques fever': 'Infectious Diseases',\n",
        "    'Quinsy': 'Ent Specialist',\n",
        "    'Rabies': 'Infectious Diseases',\n",
        "    \"Raynaud's Phenomenon\": 'Rheumatologist',\n",
        "    'Repetitive strain injury': 'Orthopedic Surgeon',\n",
        "    'Rheumatic fever': 'Cardiologist',\n",
        "    'Rheumatism': 'Rheumatologist',\n",
        "    'Rickets': 'Pediatrician',\n",
        "    'Rift Valley fever': 'Infectious Diseases',\n",
        "    'Rocky Mountain spotted fever': 'Infectious Diseases',\n",
        "    'Rubella': 'Pediatrician',\n",
        "    'SARS': 'Infectious Diseases',\n",
        "    'SIDS': 'Pediatrician',\n",
        "    'Sarcoidosis': 'Pulmonologist / Lung Specialist',\n",
        "    'Sarcoma': 'Oncologist',\n",
        "    'Scabies': 'Dermatologist',\n",
        "    'Scarlet fever': 'Infectious Diseases',\n",
        "    'Schizophrenia': 'Psychiatrist',\n",
        "    'Sciatica': 'Orthopedic Surgeon',\n",
        "    'Scrapie': 'Neurologist',\n",
        "    'Scrub Typhus': 'Infectious Diseases',\n",
        "    'Scurvy': 'Nutritionist',\n",
        "    'Sepsis': 'Infectious Diseases',\n",
        "    'Sexually transmitted infections (STIs)': 'Sexologist',\n",
        "    'Shaken Baby Syndrome': 'Pediatrician',\n",
        "    'Shigellosis': 'Infectious Diseases',\n",
        "    'Shin splints': 'Orthopedic Surgeon',\n",
        "    'Shingles': 'Dermatologist',\n",
        "    'Sickle-cell anemia': 'Hematologist',\n",
        "    'Smallpox': 'Infectious Diseases',\n",
        "    'Stevens-Johnson syndrome': 'Dermatologist',\n",
        "    'Stomach ulcers': 'Gastroenterologist',\n",
        "    'Strep throat': 'Ent Specialist',\n",
        "    'Stroke': 'Neurologist',\n",
        "    'Sub-conjunctival Haemorrhage': 'Ophthalmologist',\n",
        "    'Syphilis': 'Sexologist',\n",
        "    'Taeniasis': 'Infectious Diseases',\n",
        "    'Taeniasis/cysticercosis': 'Infectious Diseases',\n",
        "    'Tay-Sachs disease': 'Geneticist',\n",
        "    'Tennis elbow': 'Orthopedic Surgeon',\n",
        "    'Tetanus': 'Infectious Diseases',\n",
        "    'Thalassaemia': 'Hematologist',\n",
        "    'Tinnitus': 'Ent Specialist',\n",
        "    'Tonsillitis': 'Ent Specialist',\n",
        "    'Toxic shock syndrome': 'Infectious Diseases',\n",
        "    'Trachoma': 'Ophthalmologist',\n",
        "    'Trichinosis': 'Infectious Diseases',\n",
        "    'Trichomoniasis': 'Sexologist',\n",
        "    'Tuberculosis': 'Pulmonologist / Lung Specialist',\n",
        "    'Tularemia': 'Infectious Diseases',\n",
        "    'Turners Syndrome': 'Geneticist',\n",
        "    'Urticaria': 'Dermatologist',\n",
        "    'Varicose Veins': 'Vascular Surgeon',\n",
        "    'Vasovagal syncope': 'Cardiologist',\n",
        "    'Vitamin B12 Deficiency': 'Nutritionist',\n",
        "    'Vitiligo': 'Dermatologist',\n",
        "    'Warkany syndrome': 'Geneticist',\n",
        "    'Warts': 'Dermatologist',\n",
        "    'Yaws': 'Dermatologist',\n",
        "    'Yellow Fever': 'Infectious Diseases',\n",
        "    'Zika virus disease': 'Infectious Diseases',\n",
        "    'lactose intolerance': 'Gastroenterologist',\n",
        "    'papilloedema': 'Ophthalmologist'\n",
        "}\n",
        "\n",
        "# Function to retrieve the specialization based on the disease name\n",
        "def get_specialization(disease_name):\n",
        "    return disease_to_specialization.get(disease_name, \"Specialization not found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82nWRQ1wSc_j"
      },
      "source": [
        "# Suggesting doctors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "RSWffgXASc_j"
      },
      "outputs": [],
      "source": [
        "doctors_df=doctors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "G0XIXdTdSc_j"
      },
      "outputs": [],
      "source": [
        "doctors_df['Specialization'] = doctors_df['Specialization'].apply(lambda x: [spec.strip() for spec in x.split(',')])\n",
        "# Calculate the normalized satisfaction score\n",
        "doctors_df['Normalized Satisfaction Score'] = doctors_df['Patient Satisfaction Rate(%age)'] * doctors_df['Total_Reviews']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PW_3GysjSc_k",
        "outputId": "9ae284a3-ddb7-4223-9963-2987de3c561d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final list of Symptoms used for prediction:\n",
            "\n",
            "-------------------------------------------------------------------\n",
            "coughing\n",
            "feeling tired\n",
            "sore throat\n",
            "\n",
            "\n",
            "\n",
            "Top 10 diseases predicted based on symptoms\n",
            "0 Disease name: Influenza \tProbability: 89.19%\n",
            "1 Disease name: Rubella \tProbability: 66.89%\n",
            "2 Disease name: Brucellosis \tProbability: 44.6%\n",
            "3 Disease name: Hepatitis D \tProbability: 44.6%\n",
            "4 Disease name: Fibromyalgia \tProbability: 44.6%\n",
            "5 Disease name: Asthma \tProbability: 44.6%\n",
            "6 Disease name: Congestive heart disease \tProbability: 44.6%\n",
            "7 Disease name: Botulism \tProbability: 44.6%\n",
            "8 Disease name: Strep throat \tProbability: 44.6%\n",
            "9 Disease name: Anaemia \tProbability: 44.6%\n",
            "\n",
            "More details about the disease? Enter index of disease or '-1' to discontinue and close the system:\n",
            "0\n",
            "\n",
            "Influenza\n",
            "Other names -  flu, the flu, grippe (French for flu) \n",
            "Specialty -  Infectious disease \n",
            "Symptoms -  Fever, runny nose, sore throat, muscle pain, headache, coughing, fatigue \n",
            "Usual onset -  1–4 days after exposure \n",
            "Duration -  2–8 days \n",
            "Causes -  Influenza viruses \n",
            "Prevention -  Hand washing, flu vaccines \n",
            "Medication -  Antiviral drugs such as oseltamivir \n",
            "Frequency -  3–5 million severe cases per year   \n",
            "Deaths -  >,290,000–650,000 deaths per year   \n",
            "\n",
            "Top Diseases Predicted and Doctor Recommendations:\n",
            "\n",
            "-------------------------------------------------------------------\n",
            "Most Probable Disease 1: Influenza\n",
            "Required Specialization: General Physician\n",
            "Recommended Doctor: Assoc. Prof. Dr. Suresh Kumar\n",
            "City: QUETTA\n",
            "Specialization: ['Endocrinologist', 'General Physician', 'Internal Medicine Specialist']\n",
            "Qualification: MBBS, MCPS, FCPS (Medicine), FCPS (Endocrinology)\n",
            "Experience: 15.0 years\n",
            "Reviews: 1511.0\n",
            "Satisfaction Rate: 99.0%\n",
            "Average Time to Patients: 14.0 mins\n",
            "Wait Time: 11.0 mins\n",
            "Fee: PKR 2000.0\n",
            "Hospital Address: Gillani Hopsital, Satellite Town, Quetta\n",
            "Profile Link: https://www.marham.pk/doctors/quetta/endocrinologist/assoc-prof-dr-suresh-kumar#reviews-scroll\n",
            "\n",
            "-------------------------------------------------------------------\n",
            "Most Probable Disease 2: Rubella\n",
            "Required Specialization: Pediatrician\n",
            "Recommended Doctor: Dr. Mushtaq Ahmed\n",
            "City: KARACHI\n",
            "Specialization: ['Pediatrician']\n",
            "Qualification: MBBS, DCH\n",
            "Experience: 29.0 years\n",
            "Reviews: 792.0\n",
            "Satisfaction Rate: 98.0%\n",
            "Average Time to Patients: 14.0 mins\n",
            "Wait Time: 13.0 mins\n",
            "Fee: PKR 1000.0\n",
            "Hospital Address: Child Care Clinic, near yadgar fish, Karachi\n",
            "Profile Link: https://www.marham.pk/doctors/karachi/pediatrician/dr-mushtaq-ahmed#reviews-scroll\n",
            "\n",
            "-------------------------------------------------------------------\n",
            "Most Probable Disease 3: Brucellosis\n",
            "Required Specialization: Infectious Diseases\n",
            "Recommended Doctor: Dr. Waseem Iqbal\n",
            "City: ISLAMABAD\n",
            "Specialization: ['General Physician', 'Infectious Diseases', 'Diabetologist', 'Nutritionist']\n",
            "Qualification: MBBS, MSPH, RMP\n",
            "Experience: 22.0 years\n",
            "Reviews: 363.0\n",
            "Satisfaction Rate: 98.0%\n",
            "Average Time to Patients: 22.0 mins\n",
            "Wait Time: 15.0 mins\n",
            "Fee: PKR 1500.0\n",
            "Hospital Address: No Address Available\n",
            "Profile Link: https://www.marham.pk/online-consultation/general-physician/islamabad/dr-waseem-iqbal-5476#reviews-scroll\n"
          ]
        }
      ],
      "source": [
        "# Create query vector based on symptoms selected by the user\n",
        "print(\"\\nFinal list of Symptoms used for prediction:\")\n",
        "print(\"\\n-------------------------------------------------------------------\")\n",
        "sample_x = [0 for x in range(0,len(dataset_symptoms))]\n",
        "for val in final_symp:\n",
        "    print(val)\n",
        "    sample_x[dataset_symptoms.index(val)]=1\n",
        "\n",
        "\n",
        "print(\"\\n\")\n",
        "top_three_diseases = most_probable_diseases[:3]  # Take the top 3 predicted diseases\n",
        "\n",
        "\n",
        "print(f\"\\nTop {k} diseases predicted based on symptoms\")\n",
        "topk_dict = {}\n",
        "# Show top 10 highly probable disease to the user.\n",
        "for idx,t in  enumerate(topk):\n",
        "    match_sym=set()\n",
        "    row = df_norm.loc[df_norm['label_dis'] == diseases[t]].values.tolist()\n",
        "    row[0].pop(0)\n",
        "\n",
        "    for idx,val in enumerate(row[0]):\n",
        "        if val!=0:\n",
        "            match_sym.add(dataset_symptoms[idx])\n",
        "    prob = (len(match_sym.intersection(set(final_symp)))+1)/(len(set(final_symp))+1)\n",
        "    prob *= mean(scores)\n",
        "    topk_dict[t] = prob\n",
        "j = 0\n",
        "topk_index_mapping = {}\n",
        "topk_sorted = dict(sorted(topk_dict.items(), key=lambda kv: kv[1], reverse=True))\n",
        "for key in topk_sorted:\n",
        "  prob = topk_sorted[key]*100\n",
        "  print(str(j) + \" Disease name:\",diseases[key], \"\\tProbability:\",str(round(prob, 2))+\"%\")\n",
        "  topk_index_mapping[j] = key\n",
        "  j += 1\n",
        "\n",
        "select = input(\"\\nMore details about the disease? Enter index of disease or '-1' to discontinue and close the system:\\n\")\n",
        "if select!='-1':\n",
        "    dis=diseases[topk_index_mapping[int(select)]]\n",
        "    print()\n",
        "    print(diseaseDetail(dis))\n",
        "\n",
        "# Print recommendations with proper formatting\n",
        "print(\"Top Diseases Predicted and Doctor Recommendations:\")\n",
        "for i, disease in enumerate(top_three_diseases):\n",
        "    specialization_needed = disease_to_specialization[disease]\n",
        "    print(\"\\n-------------------------------------------------------------------\")\n",
        "    print(f\"Most Probable Disease {i + 1}: {disease}\")\n",
        "    print(f\"Required Specialization: {specialization_needed}\")\n",
        "\n",
        "    # Filtering doctors based on the required specialization\n",
        "    filtered_doctors = doctors_df[doctors_df['Specialization'].apply(lambda x: specialization_needed in x)]\n",
        "    # Sort and pick the top doctor(s) based on normalized satisfaction score\n",
        "    top_doctor = filtered_doctors.sort_values(by='Normalized Satisfaction Score', ascending=False).head(1)\n",
        "\n",
        "    if not top_doctor.empty:\n",
        "        doctor = top_doctor.iloc[0]\n",
        "        print(f\"Recommended Doctor: {doctor['Doctor Name']}\")\n",
        "        print(f\"City: {doctor['City']}\")\n",
        "        print(f\"Specialization: {doctor['Specialization']}\")\n",
        "        print(f\"Qualification: {doctor['Doctor Qualification']}\")\n",
        "        print(f\"Experience: {doctor['Experience(Years)']} years\")\n",
        "        print(f\"Reviews: {doctor['Total_Reviews']}\")\n",
        "        print(f\"Satisfaction Rate: {doctor['Patient Satisfaction Rate(%age)']}%\")\n",
        "        print(f\"Average Time to Patients: {doctor['Avg Time to Patients(mins)']} mins\")\n",
        "        print(f\"Wait Time: {doctor['Wait Time(mins)']} mins\")\n",
        "        print(f\"Fee: PKR {doctor['Fee(PKR)']}\")\n",
        "        print(f\"Hospital Address: {doctor['Hospital Address']}\")\n",
        "        print(f\"Profile Link: {doctor['Doctors Link']}\")\n",
        "    else:\n",
        "        print(\"No available doctors for this specialization.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}